<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://tech.k10n.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tech.k10n.me/" rel="alternate" type="text/html" /><updated>2025-07-07T00:31:35+00:00</updated><id>https://tech.k10n.me/feed.xml</id><title type="html">CH Engineering</title><subtitle>Experience and Thinking about Software Development</subtitle><entry><title type="html">왜 쿠버네티스를 도입 했는가?</title><link href="https://tech.k10n.me/2025/07/07/why-kubernetes.html" rel="alternate" type="text/html" title="왜 쿠버네티스를 도입 했는가?" /><published>2025-07-07T00:00:00+00:00</published><updated>2025-07-07T00:16:43+00:00</updated><id>https://tech.k10n.me/2025/07/07/why-kubernetes</id><content type="html" xml:base="https://tech.k10n.me/2025/07/07/why-kubernetes.html"><![CDATA[<p>회사에서 쿠버네티스 기반으로 서비스를 약 1년정도 운영 중인 상태입니다. 2024년의 비즈니스 목표는 <strong>‘우리 Platform을 Boxing하여 고객사에 딜리버리 하고 운영하는 것’</strong>이었습니다.</p>

<p><strong>목표:</strong></p>

<blockquote>
  <p>실제 고객사의 보안 요구사항을 충족 시키며 <strong>“우리 Platform을 ‘전달’ 하고 ‘운영’ 해야 한다.” (No Inbound Only OutBound)</strong></p>

</blockquote>

<h3 id="돌아보면"><strong>돌아보면:</strong></h3>

<p>이 문제를 해결해야 할 때쯤인 23년 후반기에 팀장님 새롭게 오셨습니다. 새로오신 팀장님의 주요 커리어는 클라우드와 쿠버네티스였고 팀장님이 쿠버네티스를 잘 아신다는 이유로 쿠버네티스를 이용해 해결하고자 했습니다. 결론적으로 총 2개의 고객사에 쿠버네티스를 기반의 플랫폼 Boxing하여 딜리버리하였고 프로젝트를 성공적으로 운영하여 25년에도 다른 프로젝트들을 함께하게 되었습니다.</p>

<p>하지만 이 글의 트리거 라고 볼 수 있는데 지금 생각해보면 아쉬운 점이 존재합니다. <strong>“왜 쿠버네티스를 사용했나요?”</strong> 라고 질문한다면 논리적으로 대답하기 어렵고 스스로도 납득하기도 어렵습니다. 따라서 이 글을 작성하며 조금더 학습하며 앞으로  0 → 1 을 넘어 1 → 10 경험을 쌓을 수 있게 노력해볼 예정입니다.</p>

<hr />

<h1 id="쿠버네티스란-무엇인가"><strong>쿠버네티스란 무엇인가?</strong></h1>

<p>먼저, 쿠버네티스란 무엇이고 어떤 문제를 해결하기 위해 등장한 기술인지 알아보고자 합니다.</p>

<blockquote>
  <p><em>Kubernetes is an open-source container orchestration system for automating software deployment, scaling, and management</em><br />
by Wiki</p>
</blockquote>

<p>쿠버네티스에 대한 정의를 위키 백과에서는 <strong><em>“소프트웨어의 자동화된 배포와 스케일링 그리고 운영을 위한 오픈 소스 컨테이너 오케스트레이션 시스템이라 정의합니다.”</em></strong>  즉, 쿠버네티스는 자동화된 컨테이너 운영 관리 도구인 것입니다. <strong>‘오케스트레이션’</strong> 이란 키워드가 등장하는데 그렇다면 오케스트레이션이란 무엇이고 보편적인 자동화와는 어떤 차이가 있기에 오케스트레이션이란 단어를 사용할까요?</p>

<h3 id="automation-vs-orchestration"><strong>Automation vs Orchestration</strong></h3>

<p><strong>Automation 은 무언가를 새롭게 ‘배포’하고 메트릭을 ‘감지’ 하는 것에 집중합니다.</strong>  즉, 자동화는 손쉽게 배포하고 이상 지표를 자동으로 감지할 수 있는 시스템을 만드는 것까지가 목표입니다. 하지만 운영/관리를 위해서는 보편적인 자동화로는 한계가 존재하는데 보편적인 자동화만으로는 이상 지표가 감지 되었을 때, 적절한 ‘후속 조치’를 사람이 진행해야 한다는 한계가 있습니다.</p>

<h3 id="수동-조취-케이스"><strong>[수동 조취 케이스]</strong></h3>

<ol>
  <li>컨테이너 내려간 경우, 서버에 접근하여 문제 있는 컨테이너를 제거하고 다시 컨테이너를 올려주어야 한다.</li>
  <li>트래픽 증가로 인한 부하가 늘어나는 경우, 수동으로 컨테이너 수를 증가시킨다.</li>
</ol>

<p>이처럼 서비스를 운영 시 문제가 생겼을 때 후속 조취 작업은 필수인데 이러한 작업이 ‘사람’에 의존적이라면 실수할 여지가 있으며 이는 곧 ‘ 서비스 품질이 사람에 의해 바뀔 수 있다는 것입니다.’ <strong>보편적인 자동화는 한계가 있고 이는 우리가 추구하는 완벽한 의미에서 자동화는 아닙니다. 따라서 O<em>rchestration</em>은 후속 조치에 대한 것까지 자동화 해주는 것을 목표로 등장하였고 컨테이너 환경에서는 필수적인 기능들이 되었습니다.</strong></p>

<p>쿠버네티스는 오케스트레이션 도구로서 ‘Self Healing’ 을 통해 컨테이너를 지속적으로 정상 상태로 복구시켜주고 부하가 늘어나 처리량이 떨어지면, HPA를 통해 ‘Auto Scaling’<strong>을 셀프로</strong> 진행합니다. 이렇게 <strong>쿠버네티스는 사람이 진행하던 ‘후속 조취’를 자체적으로 제공하기 때문에 휴먼 에러로 인한 품질이 저하가 발생하지 않으며, 보다 완벽한 자동으로 제공하기 때문에 엔지니어의 운영 코스트를 절감 시켜 다른 생산적인 작업에 리소스를 투입할 수 있게 해줍니다.</strong></p>

<h3 id="그-외-어떤-도구들이-있을까"><strong>그 외 어떤 도구들이 있을까?</strong></h3>

<p>쿠버네티스는 컨테이너를 오케스트레이션 하는 도구이며 쉽게 생각하면 컨테이너 관리 도구입니다. 따라서 컨테이너 관리를 위해 꼭 쿠버네티스만을 사용해야 하는 것은 아닙니다. 오히려 단순 컨테이너를 관리하는 것이 목적이면 꼭 쿠버네티스를 사용할 필요는 없으며 대표적으로 아래의 대안들이 존재합니다.</p>

<ol>
  <li>Docker Compose</li>
  <li>Docker Swarm</li>
  <li>AWS ECS</li>
  <li>서버리스 서비스들</li>
</ol>

<p>이렇게 가장 간단한 Docker Compose 부터 AWS의 ECS 그리고 다양한 서버리스 제품들까지 대안이 존재합니다. 실제로 저희도 Docker Compose만으로 간단한 3tier architecture를 사용했었습니다. 하지만 Docker Compose 만으로는 위 문제들을 해결할 수 없었고 컨테이너 기반으로 서비스를 더 잘 운영하기 위해선 쿠버네티스가 많은 도움을 준다고 생각하여 쿠버네티스를 도입하였습니다.</p>

<p>추가로 클라우드 환경이라면 컨테이너 오케스트레이션 도구로 AWS ECS 또한 강력합니다. 하지만 ECS는 결국 AWS 의존성이 높고 이식성이 떨어지며 쿠버네티스의 서비스와 같은 로드밸런서의 부재로 컨테이너 레벨에서 로드밸런싱과 서비스 디스커버리 측면에서 한계가 뚜렷합니다.</p>

<hr />

<h1 id="그렇다면-무엇이-쿠버네티스를-표준으로-만들었고-우리가-쿠버네티스를-사용하는-이유">그렇다면 무엇이 쿠버네티스를 표준으로 만들었고 우리가 쿠버네티스를 사용하는 이유!</h1>

<p>대안들이 존재하지만 쿠버네티스는 컨테이너 오케스트레이션 표준이되었다. 무엇이 컨테이너 오케이스트레이션 도구에서 표준으로 자리 잡게 했을까요?</p>

<p><img src="/assets/img/kubernetes.png" alt="Image" /></p>

<ol>
  <li><strong>높은 이식성과 쿠버네티스 에코시스템:</strong>
    <ol>
      <li>쿠버네티스를 사용하면 다양한 클라우드 환경 및 온프레미스 환경에서도 일관된 방식으로 애플리케이션을 배포하고 운영할 수 있게 해줍니다.</li>
      <li>쿠버네티스는 CNCF에서 관리하는 오픈소스이기 때문에 기능들이 빠르게 확장되고 있으며 커뮤니티기반의 강력한 에코 시스템을 확보했습니다. 이러한 에코 시스템들은 운영에 대한 코스트를 낮춥니다.</li>
    </ol>
  </li>
  <li><strong>클러스터링:</strong>
    <ol>
      <li>쿠버네티스는 Control Plane, Data Plane으로 크게 두 가지 클러스터링을 지원합니다. 따라서 하나의 노드가 Down되어도 나머지 정상 노드를 활용해 비즈니스를 가동시킬 수 있습니다.</li>
    </ol>
  </li>
  <li><strong>유연한 스케줄링:</strong>
    <ol>
      <li>쿠버네티스는 단순한 스케줄링이 아니라 Node의 상태에 따라 최선의 스케줄링을 하고자 노력합니다.</li>
      <li>쿠버네티스는 Application의 특성을 고려해 유연한 스케줄링을 제공합니다. 예를 들어 AI 기능이 필요하다면 GPU가 설치된 노드에 배치하는 것이 가능하며, 토폴로지를 제공해 내결함성을 높일 수 있습니다.</li>
    </ol>
  </li>
  <li><strong>손쉬운 네트워크 관리:</strong>
    <ol>
      <li>쿠버네티스는 CNI 플러그인이라는 가상의 네트워크를 활용해 컨테이너간 통신 컨테이너에 IP 할당 등등 다양한 네트워크 문제를 외부 의존성 없이 자체적으로 해결할 수 있게 지원합니다.</li>
    </ol>
  </li>
  <li><strong>서비스 디스커버리와 로드 밸런싱:</strong>
    <ol>
      <li>Pod들이 동적으로 생성되고 소멸되기 때문에, 서비스 간 통신을 IP 기반이 아닌 DNS 기반으로 통신할 수 있게 지원합니다.</li>
      <li>쿠버네티스는  로드 밸런싱을 이용해 Reliability를 높일 수 있는데, Not Ready 상태의 Pod들은 트래픽에서 자동으로 제외시켜 고객의 실패 경험을 최소화 시킬 수 있습니다.</li>
    </ol>
  </li>
  <li><strong>셀프 힐링과 오토스케일링:</strong>
    <ol>
      <li>내부적으로 제어루프 개념이 존재하기 때문에 Pod가 죽었을 때와 같이 Desired State에서 달라졌을 때 자동으로 Pod를 생성하는 등의 <strong>현재 상태가 목표 상태와 일치하도록 시스템을 조정</strong>하는 방식으로 작동한다.</li>
      <li>Application의 처리량이 떨어져 Latency가 늘어난다면 HPA, VPA를 사용해 탄력적인 스케일 인/아웃 을 지원합니다. 나아가 Node의 Resource Pressure에도 Cluster Autoscaler를 사용해 노드 자체도 동적으로 조절할 수 있습니다.</li>
    </ol>
  </li>
  <li><strong>볼륨 관리:</strong>
    <ol>
      <li>PV와 PVC를 분리하여 스토리지 기술과 애플리케이션의 커플링을 줄입니다. 이를 통해 컨테이너는 Row Level의 Storage 기술에 종속적이지 않고 PVC 만을 사용해 유연하게 볼륨을 구축할 수 있습니다.</li>
      <li>이러한 특징은 이식성을 높이며, 비교적 비용이 높은 Raw Level의 기술 변경에도 안정성을 제공합니다.</li>
    </ol>
  </li>
  <li><strong>최적화된 GitOps:</strong>
    <ol>
      <li>쿠버네티스는 Desired State를 유지하고자 합니다. 또 이러한 Desired State는 YAML 파일을 사용하여 선언적인 방식으로 관리됩니다.</li>
      <li>따라서 형상 코드의 형태로 (IaC) Git을 <strong>Source of Truth로</strong> 이용해 관리하며 손쉬운 롤백과 히스토리 추적을 지원하여 신뢰성을 높일 수 있습니다.</li>
    </ol>
  </li>
</ol>

<p>이러한 매커니즘들은 결국 컨테이너 환경에서 서비스를 운영할 때 고민해야하는 것들입니다. 이러한 고민들에 대한 해답을 쿠버네티스가 가장 잘 지원하기 때문에 쿠버네티스가 핵심이자 표준이 되었다 생각합니다.</p>

<hr />

<h2 id="다시-생각해보는--우리는-왜-쿠버네티스를-선택했나">다시 생각해보는,  ‘우리는 왜 쿠버네티스를 선택했나’</h2>

<p>위에서 쿠버네티스가 무엇이고 왜 표준이 되었는지 쿠버네티스의 강점들을 정리했습니다. 물론 쿠버네티스가 ‘실버 불릿’은 아니기 때문에 트레이드 오프가 발생하지만, 지금 다시 의사결정을 하라고 해도 쿠버네티스를 선택할 것 같습니다. 그렇다면 왜 그런 선택을 할지 한 번 정리해보겠습니다.</p>

<p>만약, 지금 다시 의사결정을 해야 한다면 Docker Swarm과 비교해볼 수 있을 것 같습니다. Docker Swarm은 Docker engine에 내장되어있는 Cluster management, Orchestration Tool이며 Kubernetes 처럼 클러스터링을 지원하며 어느정도의 셀프 힐링도 지원합니다. 하지만 도커스웜은 소규모 서비스에 적합하다 라는 내용을 쉽게 볼 수 있습니다. 왜 그럴까요?</p>

<h3 id="도커-스웜의-한계점"><strong>[도커 스웜의 한계점]</strong></h3>

<ol>
  <li>
    <p><strong>서비스 신뢰성을 보장하기엔 부족한 기능들:</strong> 서비스의 신뢰성을 위해선 <strong>‘고가용성’</strong> 과 <strong>‘확장성’</strong>이 중요한 포인트입니다. 도커 스웜은 컨테이너 레벨에서의 복구를 지원하지만 많이 부족합니다. (PDB도 없음)
무엇보다 컨테이너에 대한 <code class="language-plaintext highlighter-rouge">Health Check</code>, <code class="language-plaintext highlighter-rouge">Readiness</code> 기능이 없기 때문에 겉으로는 문제가 없지만 내부족으로 컨테이너가 비정상적으로 동작하고 있을 때, 트래픽이 문제의 컨테이너에도 전달되기 때문에 트래픽이 지속적으로 실패할 수 있습니다. 반면, K8s는 자동으로 비정상 파드를 트래픽 대상에서 지워 실패 경험을 최소화 합니다.</p>

    <p>또, 노드 레벨에서 장애가 났을 때 조취하는 것이 부족합니다. 쿠버네티스는 <code class="language-plaintext highlighter-rouge">Unready</code> 상태의 노드가 발견되면 즉시,  정상적인 상태의 노드로 파드의 스케줄링을 다시합니다. 이렇게 K8s는 신뢰성을 높일 수 있는 여러가지 자동화 매커니즘이 존재하지만 도커 스웜은 부족합니다.</p>
  </li>
  <li>
    <p><strong>부족한 리소스 타입:</strong> <code class="language-plaintext highlighter-rouge">StatefulSet</code><strong>,</strong> <code class="language-plaintext highlighter-rouge">DaemonSet</code>, <code class="language-plaintext highlighter-rouge">Job</code> 등등 쿠버네티스에는 단순 컨테이너를 넘어 컨테이너를 활용해 보다 다양한 종류의 서비스를 운영할 수 있습니다. 컨테이너는 Stateless 하다는 특징이 있는데 이는 큰 장점이지만 단점이 될 때도 있습니다.</p>

    <p>예를 들어 DB 같은 서비스는 Stateful 해야 하며 Stateful한 리소스들의 고유성을 보장하고 몇번을 스케줄링해도 동일한 노드에 스케줄링 해줍니다. 하지만 도커 스웜은 이러한 기능들이 존재하지 않아 애플리케이션 성격에 따라 유연한 대처가 불가능합니다.</p>
  </li>
  <li>
    <p><strong>부족한 배포 시스템:</strong> Kubernetes에는 <code class="language-plaintext highlighter-rouge">Deployment</code> 를 이용해 <code class="language-plaintext highlighter-rouge">RollingUpdate</code>를  기본적으로 지원합니다. Docker Swarm도 service update로 가능하지만 세밀한 제어가 부족합니다. 쿠버네티스는 graceful down을 지원할 수 있으며, <code class="language-plaintext highlighter-rouge">maxUnavailable</code> 를 통해 최소로 유지할 파드를 선언해, 서비스의 순단을 방지합니다.</p>
  </li>
  <li>
    <p><strong>스케줄링 유연도가 떨어진다:</strong> 쿠버네티스는 어피니티를 통해 선호하는 곳으로 컨테이너를 스케줄링할 수 있게 도우며 토폴로지를 지원해 분산 배치를 지원 하지만 도커 스웜에는 이러한 기능이 없습니다. 나아가 Resource에 대한 Request가 없어 최소한의 자원을 보장받는 것도 어렵습니다.</p>

    <p>그 외 Node가 Pressure를 받을 경우, Evict 되어야할 필요가 있는데 이 경우, QoS 클래스라는 개념을 이용해 Evict 우선 순위를 결정하여 주요 서비스의 컴퓨팅 리소스를 최대한 안전하게 지킬 수 있습니다.</p>
  </li>
</ol>

<p>이렇게 도커 스웜은 많은 부분에서 오케스트레이션 도구의 표준이 되기 어려운 형태입니다. 이러한 부족함은 고객사의 폐쇄망에 서비스를 전달하고 운영하는데 있어 신뢰도를 갖추기 어렵다. 판단됩니다. 따라서 지금에서 의사결정을 해도 쿠버네티스를 선택할 것 같습니다.</p>

<hr />

<h2 id="어떤-트레이드-오프를-가져야할까---앞으로의-숙제">어떤 트레이드 오프를 가져야할까? - 앞으로의 숙제</h2>

<p>지금까지 쿠버네티에 대해 정리하고 도커 스웜과 비교하며 쿠버네티스 라는 의사결정 배경을 고민해봤습니다. 아시겠지만 쿠버네티스를 도입했다고 끝이 아닙니다. 쿠버네티스는 복잡도가 높으며 특히나 EKS처럼 Managed가 아니기 때문에 많은 부분을 신경써야만 보다 안정적인 운영이 가능해집니다.</p>

<p><strong>가시성을 높여야합니다.</strong></p>
<ul>
  <li>비즈니스를 담당하는 애플리케이션의 가시성, 클러스터 운영의 Core인 <code class="language-plaintext highlighter-rouge">Control Plane</code> 에 대한 가시성 모두 챙겨야합니다. 관리하는 Workload들이 늘어날 수록 <code class="language-plaintext highlighter-rouge">Control Plane</code> 의 부하는 높아지고 Control Plane이 죽으면 전체 장애로 이어집니다.</li>
</ul>

<p><strong>물리 머신 레벨에서도 최대한 무중단를 지원해야 합니다.</strong></p>
<ul>
  <li>
    <p>쿠버네티스 버전 업, 노드의 장애로 인한 교체 과정에서 비즈니스는 자체가 완전히 멈추는 것을 피하고자 최대한 무중단을 지원할 수 있어야 합니다.</p>
  </li>
  <li>
    <p>이를 통해 클러스터에 대한 MTTR를 높이고 SLA를 준수할 수 있게 노력해야 합니다.</p>
  </li>
</ul>

<hr />

<h2 id="마무리">마무리</h2>

<p>지금까지 머리속에 뒤죽박죽으로 있던 것들을 정리해봤습니다. 이 과정에서 처음 목표 했던 Why에 대한 답변도 생긴 것 같습니다. 특히, 도커 스웜의 한계점을 명확히 이해하고 우리가 왜 쿠버네티스를 선택해야 했는지 보다 명확해진 시간이었던 것 같습니다.</p>

<p>앞으로는 쿠버네티스 클러스터 구축의 모범 사례도 학습하며 우리의 약점들을 어떻게 보완할 수 있을지 고민해보도록 할 예정입니다.</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="DevOps" /><summary type="html"><![CDATA[쿠버네티스를 도입한 이유를 보다 깊이 고민해보았습니다.]]></summary></entry><entry><title type="html">나도 eBPF 프로그램을 만들어 볼 수 있을까?</title><link href="https://tech.k10n.me/blog/2024/08/25/welcome-to-jekyll.html" rel="alternate" type="text/html" title="나도 eBPF 프로그램을 만들어 볼 수 있을까?" /><published>2024-08-25T01:53:02+00:00</published><updated>2025-07-07T00:16:43+00:00</updated><id>https://tech.k10n.me/blog/2024/08/25/welcome-to-jekyll</id><content type="html" xml:base="https://tech.k10n.me/blog/2024/08/25/welcome-to-jekyll.html"><![CDATA[<p>최근에 팀 내에서 꽤 도전적인 네트워크 관측가능성 목표들을 설정하고, 업무들을 진행해 보고 있다.
여러 워크로드로부터 발생하는 네트워크 트래픽을 좀 더 투명하게 살펴보기 위한 방법을 마련하는 것인데,</p>

<p>‘오픈소스 도구를 쓸 까.. 직접 만들어 볼 수는 없을까..’ 고민하던 중에 관심이 많았던 eBPF를 활용해서,
우리가 직접 만든 코드로 네트워크 트래픽을 측정하면 어떨까 하는 마음에 이리저리 혼자서 리서치 해 본 내용을 공유한다.</p>

<p>대부분의 클라우드 프로바이더들이 제공하는 네트워크 관측가능성 상품들은 (예. AWS VPC Flowlog) 프로바이더 계층에서 측정된 데이터를 제공해 주므로 훌륭한 도구이긴 하지만, 굉장히 비싸서 실제 환경에 도입하기에는 다소 어려운 점이 있다.</p>

<p>그리고.. 인터넷을 통해 네트워크 관측 가능성을 위한 오픈소스 도구들을 찾아보면 여러가지가 나오긴 하지만, 원하는 수준의 도구는 아직 찾지 못했다.</p>

<p>원하는 바에 가장 근접한 오픈소스 도구로는 옆 팀원분이 소개해 준 <strong><a href="https://coroot.com/">coroot</a></strong> 라는 도구가 있는데 목표를 달성하는 방식도 내가 지향하는 방향과 가깝다고 생각했다.</p>

<p>이 기술적 방향에 좀 더 확신이 생겨서 열심히 공부해 보면서, eBPF 생태계에 대해서 조금 더 알게 된 내용들을 공유한다.
스터디 결과 내용은 개인 저장소로 올려두었음. <a href="https://github.com/Ashon/_study-ebpf-and-bcc">https://github.com/Ashon/_study-ebpf-and-bcc</a></p>

<blockquote>
  <p>해당 글에서는 eBPF 기술 자체에 대한 소개는 많지 않습니다.</p>

  <p>eBPF 기술에 대한 약간의 배경지식을 가지고, 뭔가 도구를 직접 만들어 보고 싶은 분이나,
eBPF의 활용 사례들은 어떤게 있을 지 감을 잡는 분들에게 조금 더 도움이 될 것 같습니다.</p>

  <p>제가 리눅스 커널에 대한 지식이 많지 않은 상태에서 필요한 내용들을 찾아가면서 공부한 내용이다 보니, 글 내용에서는 이론적 깊이가 다소 얕을 수 있습니다.</p>

  <p>eBPF 관련 도구를 도입 할 예정이거나, 공부하려는 분들이, 이 글을 통해서 eBPF 기술을 활용하는데 조금 더 탄력이 붙을 수 있으면 좋겠습니다.</p>

  <p>다양한 의견과 오류에 대한 수정은 댓글로 알려주시면 저도 더 공부 해 보도록 하겠습니다. 감사합니다.</p>
</blockquote>

<h2 id="ebpf-가-뭔가요-에-대한-개인적인-생각">“eBPF” 가 뭔가요? 에 대한 개인적인 생각..</h2>

<p>eBPF에 대한 자세한 내용은 아래 링크를 참조.</p>

<ul>
  <li><a href="https://ebpf.io/what-is-ebpf/">What is eBPF? An Introduction and Deep Dive into the eBPF Technology</a></li>
  <li><a href="https://www.kernel.org/doc/html/v5.17/bpf/index.html#">BPF Documentation — The Linux Kernel  documentation</a></li>
</ul>

<p>리눅스 커널 내부에 사용자 프로그램을 배치해서, 커널 내부의 상태들을 사용자가 원하는 대로 파악할 수 있게 해 주는 강력한 관측 가능성 기술이다.</p>

<p>매우 강력한 만큼 eBPF를 사용하기 위한 진입 장벽도 생각보다 높은 편이긴 해서, 일반적으로는 eBPF를 활용한 오픈소스 구현체를 레버리지해서 사용하는 경우가 많은 것 같고, 내가 생각해 볼 때 시작이 어려운 이유를 좀 생각해 봤다.</p>

<ul>
  <li><strong>“eBPF”</strong> 는 커널 내부를 관측하는 기술 자체를 의미하는 용어 (BPF 기술로부터 발전)</li>
  <li><strong>“eBPF를 사용한다”</strong> 라는 말은…
    <ul>
      <li>eBPF 기술을 활용해서,</li>
      <li>내가 직접 <strong>“커널 내부의 관측가능성을 달성하는 프로그램을 구현”</strong>해 낸다는 말이라고 생각함.
        <blockquote>
          <p><del>엄밀히 말하면 관측가능성 문제만 푸는건 아님</del></p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>그러므로, 커널 내부의 어떤 부분을 관측해야 할 지 사용자(개발자)가 알아야 함.
    <ul>
      <li>‘커널이 어떻게 동작하는지를 알아야 관측할 대상을 지정할 수 있다.’
        <ul>
          <li>다른말로… ‘커널의 어떤 부분을 관측할 수 있는지 내가 알아야 한다.’
            <blockquote>
              <p><del>… 쉽지않아.</del></p>
            </blockquote>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>현대의 리눅스 커널<del>(옛날에도 그랬지만)</del>은 정말 넓고 다양한 기술들을 담고 있으므로 커널을 공부해야지 하는 생각을 하면 시작부터 머리가 아득한데.. 목표치를 낮춰서 접근 할 필요가 있음.</p>

<ul>
  <li>어차피 한사람이 모든걸 다 알수는 없음..</li>
  <li>리눅스 커널 안의 특정 부분 (네트워킹, 그 중에서도 TCP.. 예를 들자면) 을 좀 공부해 보고…</li>
  <li>이 부분의 특정 지점을 공부해서 eBPF 프로그램을 만들어 본다거나..
    <blockquote>
      <p><del>(이것도 난이도가 있긴 해…)</del></p>
    </blockquote>
  </li>
</ul>

<p>그래도 이런 식의 접근이라면.. (물론 TCP 자체를 아는것은 요즘 시대에서는 꽤 난이도가 있는 일이지만),
한번 비벼볼 만 하다고 생각함. <del>(그리고 우리에겐 ChatGPT가 있다.)</del></p>

<p>뿐만아니라, eBPF 기술 안에도 굉장히 넓은 세계가 있다.</p>

<blockquote>
  <p>XDP, 커널의 네트워킹 성능을 우회하여 초고속으로 데이터를 주고받을 수 있도록 하는 기술.. 이라거나..</p>
</blockquote>

<p>여기서는 eBPF를 이용해 <a href="https://docs.kernel.org/trace/kprobes.html">kprobe(kernel probe)</a> 를 등록해서, 커널 내부의 네트워크 가시성을 확보하기 위한 내용을 진행했다.</p>

<h2 id="ebpf를-활용해-만들어-볼-네트워크-관측-도구">eBPF를 활용해 만들어 볼 네트워크 관측 도구</h2>

<p>보편적인 http 웹서버 애플리케이션들은 리눅스 커널 TCP 스택의 tcp_sendmsg, tcp_recvmsg 시스템콜을 이용해 네트워크 통신을 수행한다. (<del>QUIC은 UDP니까 제외합니다.</del>)</p>

<blockquote>
  <p>엄격하게 따져보면 웹서버들은 더 다양한 tcp 시스템콜 사용할텐데, 소개하는 자리니까 간단히 tcp_sendmsg, tcp_recvmsg만 살펴보자.</p>
</blockquote>

<p>나는 저 tcp_sendmsg, tcp_recvmsg 함수에 kprobe를 붙여서, 여러 웹서버들이 주고받는 데이터로부터 SRC IP, DST IP, payload size 등을 측정하는 애플리케이션을 만들어 보았다.</p>

<p><img src="/assets/2024-08-25/fig1.png" alt="" /></p>

<p>작성하는 애플리케이션의 기능은 아주 단순하다.</p>

<ul>
  <li>SRC IP, DST IP 정보가 있으면, 어떤 네트워크 구간을 이동하는지 알 수 있다.</li>
</ul>

<p>하지만 단순한 기능을 활용해서 다른 모니터링 시스템과 강력한 시너지를 낼 수 있게 만들 수 있을 것이다.</p>

<ul>
  <li>이 정보를 활용하면, 특정 워크로드가 어떤 네트워크 경로로 통신을 하는지 확인할 수 있음.</li>
  <li>비용 최적화, 경로 최적화를 하는데 사용할 수 있음.</li>
  <li>장애 발생 시 네트워크 구간을 투명하게 확인해서 트러블슈팅 할 수 있게 됨.</li>
  <li>이 정보를 별도 데이터베이스로 수집하고, IP 프로파일링 도구를 활용하면 더더욱 강력한 모니터링이 가능해 진다.
    <ul>
      <li>IP profiling: k8s pod들을 확인한다거나, VM 인스턴스들의 정체, 외부 IP를 파악하는 용도</li>
    </ul>
  </li>
</ul>

<h3 id="배경지식">배경지식</h3>

<h4 id="kprobe-kretprobe---kernel-probes">kprobe, kretprobe - kernel probes</h4>

<p>kprobe는 kernel probe의 줄임말로 여러 커널 함수들의 진입점에 어떤 값이 들어가는지 확인할 수 있도록 하는 관측 기술이다. (<a href="https://docs.kernel.org/trace/kprobes.html">링크 - Kernel Probes @ kernel.org</a>)</p>

<ul>
  <li>함수 진입점을 살펴볼 때는 kprobe를 사용할 수 있고,</li>
  <li>함수 응답값을 살펴볼 때는 kretprobe를 사용하면 된다. (kernel return probe)</li>
</ul>

<p><img src="/assets/2024-08-25/fig2.png" alt="" /></p>

<p>대략… 이렇게 생겼다.</p>

<p>kprobe, kretprobe는 커널 함수 앞뒤에 사용자가 작성한 함수를 후킹하는 방식으로 관측가능성을 제공하는데.. 사용자는 아래 준비물들을 가지고 커널 내부를 관찰할 수 있다.</p>

<ul>
  <li>관찰하고자 하는 커널 함수 -&gt; <strong>어떤 함수를 디버깅 할 지 알아야 함.</strong></li>
  <li>관찰값을 어떻게 처리할 것인지 핸들링하는 함수 -&gt; <strong>후킹된 값을 어떻게 볼 지 작성해야 함.</strong></li>
</ul>

<h4 id="kernel-tracepoints---내가-확인해-볼-수-있는-커널-함수들-목록은-어디에-있지">Kernel Tracepoints - 내가 확인해 볼 수 있는 커널 함수들 목록은 어디에 있지?</h4>

<p>리눅스 커널은 문제가 발생했을 때 내부 상태를 들여다보거나, 복구하기 위한 별도의 기능을 파일시스템 형태로 제공한다.</p>

<p>대부분은 서버 내부에 <strong>/sys/kernel/debug</strong> 라는 디렉토리 하위에 debugFS가 마운트 되어 있을텐데, 여기서 내가 계측하고자 하는 커널 함수들을 확인할 수 있다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 만약 서버에 debugFS가 마운트 되어 있지 않다면, 직접 마운트 해 주면 된다.</span>
<span class="nv">$ </span><span class="nb">sudo </span>mount <span class="nt">-t</span> debugfs none /sys/kernel/debug
</code></pre></div></div>

<p>debugFS를 보면 다양한 파일들이 보이는데 일단 eBPF에서 추적 가능한 함수 목록을 찾기 위해서는 <strong>/sys/kernel/debug/tracing/</strong> 디렉토리 내의 파일들을 찾아보면 된다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 트레이싱 할 수 있는 함수 목록들이 적힌 파일들</span>
<span class="nv">$ </span>find /sys/kernel/debug <span class="nt">-name</span> avail<span class="k">*</span>

/sys/kernel/debug/tracing/available_filter_functions_addrs
/sys/kernel/debug/tracing/available_filter_functions  <span class="c"># 추적 가능한 함수 목록</span>
/sys/kernel/debug/tracing/available_tracers
/sys/kernel/debug/tracing/available_events
/sys/kernel/debug/tracing/rv/available_reactors
/sys/kernel/debug/tracing/rv/available_monitors
...
</code></pre></div></div>

<p>아래와 같이 추적가능한 함수 목록에서 필요한 함수들을 찾을 수 있다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 추적 가능한 함수 목록에서 tcp 관련 함수들 찾기</span>
<span class="nv">$ </span><span class="nb">cat</span> /sys/kernel/debug/tracing/available_filter_functions | <span class="nb">grep </span>tcp_

...
tcp_sendmsg
...
tcp_recvmsg
...
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">위에서 조회되는 함수들을 진입점으로 삼아서 커널 내부에서 수행하는 기능을 확인해 보고, 어떻게 계측할 지 결정하면 된다.</code></p>

<h4 id="bccbpf-compiler-collection---ebpf-프로그램-툴킷">BCC(BPF Compiler Collection) - eBPF 프로그램 툴킷</h4>

<p><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></p>

<p><a href="https://www.iovisor.org/">IOvisor</a> 프로젝트에서 관리되는 BPF 프로그램 툴킷인데, 이를 이용하면 사용자(개발자)는 보다 편리하게 ebpf 프로그램을 작성하고 관리할 수 있게 된다.</p>

<p>하는일을 크게 살펴봤을때,</p>

<ul>
  <li>eBPF 프로그램 코드(백엔드) 를 작성하면</li>
  <li>다양한 언어로 eBPF 프로그램(백엔드)에 대한 프론트엔드 인터페이스를 만들수 있게 해 줌.
    <ul>
      <li>eBPF 기술 + 기능을 확장.</li>
    </ul>
  </li>
</ul>

<p>이런게 가능해 진다.</p>

<ul>
  <li>커널 함수의 어떤 기능을 추적하는 eBPF 코드를 작성하고,</li>
  <li>내가 잘 아는 파이썬과 FastAPI를 이용해서,</li>
  <li><strong>eBPF 프로그램이 측정한 값을 HTTP로 노출할 수 있음.</strong></li>
</ul>

<blockquote>
  <p>브렌단 그렉이 과거에 eBPF를 소개하면서 커널계의 브라우저 Javascript Engine 같은 것이라고 소개한 적이 있다. (<a href="https://www.brendangregg.com/blog/2024-03-10/ebpf-documentary.html">링크</a>)</p>

  <p>나는 처음에 저런 비유를 드는것에 도무지 이해가 가지 않았는데, 이번에 BCC로 직접 프로그램을 작성해 보면서, 그제서야 저 비유의 의미를 조금 알게 된 것 같다..</p>
</blockquote>

<h2 id="예제-프로젝트-소개---tcp_monitor">예제 프로젝트 소개 - tcp_monitor</h2>

<p>아래는 eBPF 스터디를 진행해 보면서 작성해 본 프로젝트.</p>

<p><a href="https://github.com/Ashon/_study-ebpf-and-bcc">https://github.com/Ashon/_study-ebpf-and-bcc</a></p>

<ul>
  <li>lima로 vm 정의, 초기 프로비저닝 코드
    <ul>
      <li>VM 정의 템플릿: (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/ebpf-dev.tpl.yaml">링크</a>)</li>
      <li>VM 실행 스크립트: (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/launch.sh">링크</a>)</li>
    </ul>
  </li>
  <li>eBPF 프로그램 코드 디렉토리 (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/tree/main/workspace">링크</a>)
    <ul>
      <li>docker compose를 활용해서 빌드하고 실행할 수 있음.</li>
      <li>k8s 워커노드에 데몬셋으로 띄워서 테스트 해 보고싶긴 한데.. 일단은 좀 더 디밸롭 해 보기</li>
    </ul>
  </li>
  <li>개발환경 구축 가이드는 (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/tree/main?tab=readme-ov-file#22-define--start-development-vm-instance">링크</a>) 참고</li>
</ul>

<h3 id="준비물-lima---리눅스-가상화-도구">준비물: Lima - 리눅스 가상화 도구</h3>

<p>맥 위에서 eBPF개발을 해 보려면 리눅스 가상머신이 필요하다.</p>

<p>Lima는 MacOS 안에서 Linux VM을 구동할 수 있게 해 주는 가상화 API인데, 지금까지 파악해 본 내용으로는 가장 사용성이 좋고, 기술적으로도 많이 오픈되어 있는 것 같아서 요걸 선택했음.</p>

<p><img src="/assets/2024-08-25/fig3.png" alt="" /></p>

<p>리눅스 머신 위에서 개발할 때 VSCode Remote Development를 활용하면 좀 더 편리하게 접근할 수 있음.
이제 Lima를 가지고 Linux VM을 올려서, VM 위에서 eBPF 프로그램을 만들어 볼 수 있다.</p>

<ul>
  <li><a href="https://github.com/lima-vm/lima">Github lima-vm/lima</a></li>
  <li><a href="https://github.com/lima-vm/socket_vmnet">Github lima-vm/socket_vmnet</a></li>
</ul>

<h3 id="tcp_monitor-프로그램-자세히-살펴보기">tcp_monitor 프로그램 자세히 살펴보기</h3>

<p>이번에 작성한 프로그램을 좀 더 자세히 들여다 보면…</p>

<p><img src="/assets/2024-08-25/fig4.png" alt="" /></p>

<p>eBPF 프로그램 (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/tcp_monitor/tcp_monitor.c">Code</a>)</p>

<ul>
  <li>tcp_sendmsg 함수로 들어오는 소켓 구조체 정보에서 필요한 데이터를 계측하는 함수 작성</li>
  <li>tcp_recvmsg 함수로 들어오는 소켓 구조체 정보에서 필요한 데이터를 계측하는 함수 작성</li>
  <li>각 함수에서 src, dst, tcp payload 크기를 추출해서 BPF HASH에 저장.</li>
</ul>

<p>파이썬 프로그램 (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/tcp_monitor/tcp_monitor.py">Code</a>)</p>

<ul>
  <li>eBPF 프로그램을 로드하고, 커널 함수에 어태치 함.</li>
  <li>BPF_HASH에서 주기적으로 정보를 추출해서 python dict로 저장</li>
  <li>FastAPI는 트래픽 정보를 저장하는 python dict를 응답하는 API가 존재함.</li>
</ul>

<p>컨테이너 정의 (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/tcp_monitor/Dockerfile">Code</a>)</p>

<ul>
  <li>BCC로 작성 된 프로그램을 컨테이너로 빌드할 수 있음.</li>
  <li>런타임 정의는 docker compose를 활용. (<a href="https://github.com/Ashon/_study-ebpf-and-bcc/blob/main/tcp_monitor/docker-compose.yml">Code</a>)
    <ul>
      <li>K8s로 배포 정의를 작성할 때 참고하면 됨.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>코드를 들여다 보면 “MAX_ENTRIES” 나, “ktime_map” 이라는 값들을 볼 수 있는데, 이는 트래픽 계측을
좀 더 정확하고 안전하게 하기 위한 부가 장치들이므로 코드 작성 내용에서는 설명을 하지 않는다.</p>
</blockquote>

<h3 id="bpf-프로그램-상태-조회">BPF 프로그램 상태 조회</h3>

<p>bpftool을 활용하면 커널에 로드된 BPF 프로그램들과 프로그램이 사용중인 데이터들을 쉽게 확인해 볼 수 있다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># linux 툴 설치 (우분투를 예시로.. cent계열은 패키지 따로 찾아보세용~)</span>
<span class="nv">$ </span><span class="nb">sudo </span>apt <span class="nb">install </span>linux-tools-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span>

<span class="c"># bpf program list 조회</span>
<span class="nv">$ </span><span class="nb">sudo </span>bpftool prog list

...

184: kprobe  name poll_sendmsg  tag 8d470dd2a021dc12  gpl
<span class="c">#                 ^^^^^^^^^^^^ 내가 작성한 함수의 이름</span>
    loaded_at 2024-08-22T20:51:51+0900  uid 0
    xlated 624B  jited 560B  memlock 4096B  map_ids 6,8
<span class="c">#                                           ^^^^^^^^^^^ 해당 함수가 사용하는 BPF MAP ID</span>
    btf_id 81
185: kprobe  name poll_recvmsg  tag 9e1707d2f89dc165  gpl
    loaded_at 2024-08-22T20:51:51+0900  uid 0
    xlated 624B  jited 560B  memlock 4096B  map_ids 6,7
    btf_id 81
</code></pre></div></div>

<p>작성한 ebpf 프로그램들이 잘 올라간 것을 확인할 수 있다.</p>

<h3 id="bpf-프로그램에서-사용중인-데이터-살펴보기">BPF 프로그램에서 사용중인 데이터 살펴보기</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># BPF 프로그램이 사용중인 map들 살펴보기</span>
<span class="nv">$ </span><span class="nb">sudo </span>bpftool map list

...

7: <span class="nb">hash  </span>name recv_bytes  flags 0x0
    key 12B  value 16B  max_entries 300000  memlock 32389744B
    btf_id 81
8: <span class="nb">hash  </span>name send_bytes  flags 0x0
    key 12B  value 16B  max_entries 300000  memlock 32389744B
    btf_id 81
</code></pre></div></div>

<p>tcp_monitor가 사용중인 recv_bytes, send_bytes 맵들이 잘 올라간 것을 볼 수 있음.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 직접 map id를 덤프해서 어떤 데이터가 매핑되어 있는지도 확인할 수 있다.</span>
<span class="c"># 동작중인 애플리케이션 맵을 자세히 관찰해 볼 수 있음.</span>
<span class="nv">$ </span><span class="nb">sudo </span>bpftool map dump <span class="nb">id </span>8
<span class="o">[{</span>
        <span class="s2">"key"</span>: <span class="o">{</span>
            <span class="s2">"src_ip"</span>: 252029120,
            <span class="s2">"dst_ip"</span>: 33925312,
            <span class="s2">"src_port"</span>: 22,
            <span class="s2">"dst_port"</span>: 54357
        <span class="o">}</span>,
        <span class="s2">"value"</span>: <span class="o">{</span>
            <span class="s2">"bytes"</span>: 21120,
            <span class="s2">"timestamp"</span>: 740363868867
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">]</span>
</code></pre></div></div>

<p>bpftool 바이너리를 활용해서 프로그램 상태를 투명하게 볼 수 있는데…</p>

<p>이 말은 굳이 BCC를 사용하지 않더라도, eBPF 프로그램 코드를 Native 로 작성하고, 다른 외부 언어들을 활용해서 BPF MAP을 통해서 데이터를 주고받을 수 있게 됨을 의미한다.</p>

<h2 id="소감">소감</h2>

<p>지금까지,</p>

<ul>
  <li>ebpf에 대한 간단한 소개</li>
  <li>bcc를 활용해서 ebpf 프로그램을 만드는 방법</li>
  <li>bpf 프로그램들의 상태를 추적해 보는 방법</li>
</ul>

<p>들을 알아보았는데…</p>

<ul>
  <li>eBPF는 접근하기 꽤 어려운 기술이긴 하지만.. 요즘에 다양한 학습 도구가 있어서, 꽤 편리하게 접근해 볼 수 있게 되었다.
    <blockquote>
      <p><del>우리에겐 ChatGPT가 있다.</del></p>
    </blockquote>
  </li>
  <li>세상에 다양한 관측가능성 도구들이 있지만…
    <ul>
      <li>eBPF를 활용해서 우리에게 최적화된 메트릭을 수집할 수 있을 것 같은 가능성을 보았음.</li>
    </ul>
  </li>
  <li>BCC는 매우 훌륭하게 eBPF 프로그램을 확장할 수 있게 해 주는 것 같다.</li>
</ul>

<p>이번 내용을 발판삼아 직접 eBPF 작성도 해 보고, 다른 eBPF 프로젝트를 만나면 반가운 마음(?)으로, 코드를 들여다 보면서 또 한 수 배울 수 있도록 하면 좋을 것 같다.</p>

<h2 id="함께보면-좋은-내용">함께보면 좋은 내용</h2>

<ul>
  <li><a href="https://ebpf.io/what-is-ebpf/">What is eBPF? An Introduction and Deep Dive into the eBPF Technology</a></li>
  <li><a href="https://www.kernel.org/doc/html/v5.17/bpf/index.html#">BPF Documentation — The Linux Kernel  documentation</a></li>
  <li><a href="https://blog.cloudflare.com/ko-kr/how-to-drop-10-million-packets-ko-kr">Cloudflare - 초당 천만개의 패킷을 버리는 방법</a>
    <ul>
      <li>eBPF의 XDP 기술을 활용해서 커널 네트워킹 스택에 들어오기도 전에 packet을 다 쳐내버리는 이야기</li>
    </ul>
  </li>
  <li><a href="https://blog.cloudflare.com/introducing-ebpf_exporter">Cloudflare - Introducing ebpf_exporter</a>
    <ul>
      <li>Cloudflare에서 만든 ebpf exporter 를 소개하는 내용</li>
      <li><a href="https://github.com/cloudflare/ebpf_exporter">Github - cloudflare/ebpf_exporter</a></li>
    </ul>
  </li>
</ul>

<p>BCC 관련 내용</p>

<ul>
  <li><a href="https://github.com/iovisor/bcc/blob/master/src/cc/export/helpers.h">bcc helpers</a></li>
  <li><a href="https://github.com/iovisor/bcc/blob/master/src/cc/export/proto.h">bcc protocol structures</a></li>
</ul>

<p>XDP 애플리케이션 예제들</p>

<ul>
  <li><a href="https://github.com/ashhadsheikh/ebpf/blob/master/router.c">XDP로 만든 Router</a></li>
  <li><a href="https://github.com/Netronome/bpf-samples/tree/a7dd7d8c38636dd0a1bb5f2380ce309f314abfe3/l4lb">XDP L4LB 예제</a>
    <ul>
      <li>널리 알려진 오픈소스로는 <a href="https://github.com/facebookincubator/katran">Facebook의 Katran</a> 이 있음</li>
    </ul>
  </li>
  <li><a href="https://github.com/iovisor/bcc/blob/master/examples/networking/xdp/xdp_redirect_cpu.py">XDP를 이용해 패킷처리를 특정 CPU Core에만 할당하도록 처리하는 예제</a></li>
</ul>

<p>그 외 eBPF를 활용한 오픈소스들</p>

<ul>
  <li><a href="https://l3af.io/">L3AF</a>: Linux Foundation에서 관리하는 eBPF Marketplace project.
    <blockquote>
      <p>지금의 Docker 레지스트리같이, eBPF 프로그램들도 마켓플레이스를 통해 다양한 기능들이 제공될 것이라 생각한다.
눈여겨 보는 프로젝트 중 하나.</p>
    </blockquote>
  </li>
  <li><a href="https://cilium.io/">cilium</a>: eBPF XDP 기반 K8s CNI.</li>
  <li>eBPF 기반 관측가능성 도구
    <ul>
      <li><a href="https://github.com/coroot/coroot">coroot</a></li>
      <li><a href="https://github.com/microsoft/retina">retina</a></li>
      <li><a href="https://github.com/sustainable-computing-io/kepler">kepler</a></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="blog" /><category term="Network" /><category term="Linux" /><category term="eBPF" /><summary type="html"><![CDATA[리눅스 커널 내부 TCP 통신을 위한 시스템콜을 추적하는 간단한 eBPF 프로그램을 직접 작성해 보면서, eBPF 개발 생태계와 주변에 어떤것들이 있는지 공부해 본 내용을 공유한다.]]></summary></entry></feed>